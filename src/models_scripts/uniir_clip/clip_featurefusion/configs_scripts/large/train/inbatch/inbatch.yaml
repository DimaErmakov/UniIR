# Train CLIPScoreFusion model on MBEIR dataset
# <-- Important! Change this for each experiment.
experiment:
    instruct_status: "Instruct"
    exp_name: "InBatch"
    description: "${model.name} ${model.size} ${experiment.instruct_status} ${experiment.exp_name}"
    path_suffix: "${model.short_name}/${model.size}/${experiment.instruct_status}/${experiment.exp_name}/"

# WandB settings
wandb_config:
    enabled: True
    experiment_name: "${experiment.description}"

# Logger settings
logger_config:
    logger_out_dir: "logger/${experiment.path_suffix}"  # logger will be saved to mbeir_dir/logger/experiment.path_suffix
    logger_out_file_name: "train.log"  #TODO: add date to log file name

# Dataset settings
data_config:
    image_size: 224, 224
    hard_neg_num: 0
    in_batch_neg_num: 0  # TODO: Move this to model config
    shuffle_cand: True
    
    returns: null

    # Relative to mbeir_data_dir
    query_instruct_path: "instructions/query_instructions.tsv"
    cand_instruct_path: "instructions/candidate_instructions.tsv"

    train_query_data_path: "train/union_train/mbeir_union_up_train.jsonl"
    train_cand_pool_path: "cand_pool/union_pool/mbeir_union_train_cand_pool.jsonl"

    val_query_data_path: "val/union_val/mbeir_union_val.jsonl"
    val_cand_pool_path: "cand_pool/union_pool/mbeir_union_val_cand_pool.jsonl"

# DataLoader settings
dataloader_config:
    num_workers: 5
    train_batch_size: 90  # 78597MiB / 81559MiB
    valid_batch_size: 1048

# Trainer settings
trainer_config:
    gradient_accumulation_steps: 1
    num_train_epochs: 20
    learning_rate: 1e-5
    t5_learning_rate: 5e-4
    warmup_steps: 0
    eval_steps: 500
    early_stop: 5  # TODO: we are not using this.
    print_freq: 50
    weight_decay: 0.5 #TODO: we are not using this.

# Evaluator settings
evaluator:
    enable_eval: False
    eval_freq: 5
    print_freq: 10

# Model settings
model:
    name: "CLIPFeatureFusion"
    short_name: "CLIP_FF"
    size: "Large"

    clip_vision_model_name: "ViT-L/14"
    pretrained_clip_model_dir: "mbeir_checkpoint/CLIP/" # CLIP will be loaded from mbeir_dir/mbeir_checkpoint/CLIP
    gather_embeddings: True

    ckpt_config:
        ckpt_dir: "mbeir_checkpoint/${experiment.path_suffix}" # ckpt will be saved to mbeir_dir/mbeir_checkpoint/experiment.path_suffix
        resume_training: False
        ckpt_name: ""


# Random seed
seed: 2023

# Distributed training settings
dist_config:
    dist_url: "env://"